---
title: "ESM244 Lab 8 Part 2"
author: "Anna Abelman"
date: "2/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)
```

```{r}
#read in the report
ipcc_path <- here("data", "ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)

ipcc_p9 <- ipcc_text[9]
ipcc_p9
#any time there is a /r/n that is a line break in the actual text
```

#### Get this into df shape + do some wrangling

 - Split up pages into separate lines (using `/n` or `/r/n`) using `stringr::str_split()`
 - unnest into regulat columns using `tidyr::unnest()`
 - removing leading/trailing white space using `stringr::str_trim()`
```{r}
ipcc_df <- data.frame(ipcc_text) %>% 
  mutate(text_full = str_split(ipcc_text, pattern = "\\n")) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full))
```

#### Get tokens using `unnest_tokens()`

```{r}
ipcc_token <- ipcc_df %>% 
  unnest_tokens(word, text_full)
```

#### Count all the words

```{r}
ipcc_wc <- ipcc_token %>% 
  count(word) %>% 
  arrange(-n)
```

#### Remove the stop words

```{r}
ipcc_stop <- ipcc_token %>% 
  anti_join(stop_words) %>%
  dplyr::select(-ipcc_text)
```

Remove all numeric pieces
```{r}
ipcc_no_numeric <- ipcc_stop %>% 
  dplyr::filter(is.na(as.numeric(word)))
```

#### Start doing some visualization

Word cloud:
```{r}
ipcc_top100 <- ipcc_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)

ipcc_cloud <- ggplot(data = ipcc_top100, aes(label = word))+
  geom_text_wordcloud()+
  theme_minimal()
ipcc_cloud

ggplot(data = ipcc_top100, aes(label = word, size = n))+
  geom_text_wordcloud_area(aes(color = n), 
                           shape = "circle")
```

### Sentiment analysis

First, check out the ‘sentiments’ lexicon. From Julia Silge and David Robinson (https://www.tidytextmining.com/sentiment.html):

“The three general-purpose lexicons are

  -  AFINN from Finn Årup Nielsen,
  -  bing from Bing Liu and collaborators, and
  -  nrc from Saif Mohammad and Peter Turney

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon."

Let's explore the sentiment lexicons. "bing" included, other lexicons ("afinn", "nrc", "loughran") you'll be prompted to to download.

**WARNING:** These collections include very offensive words. I urge you to not look at them in class.

"afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos
```
bing: binary, "positive" or "negative"
```{r}
get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "nrc")
```
### Sentiment analysis with afinn: 

First, bind words in `ipcc_stop` to `afinn` lexicon:
```{r}
ipcc_afinn <- ipcc_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Find counts of value rankings
```{r}
ipcc_afinn_hist <- ipcc_afinn %>% 
  count(value)

ggplot(data = ipcc_afinn_hist, aes(x = value, y = n))+
  geom_col()
```

```{r}
ipcc_afinn2 <- ipcc_afinn %>% 
  filter(value == 2)
```

```{r}
ipcc_summary <- ipcc_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

#### Chek out sentiments by NRC

```{r}
ipcc_nrc <- ipcc_stop %>% 
  inner_join(get_sentiments(lexicon = "nrc"))

#see what's excluded
ipcc_exclude <- ipcc_stop %>% 
  anti_join(get_sentiments(lexicon = "nrc"))
```

Find counts by sentiment
```{r}
ipcc_nrc_n <- ipcc_nrc %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  mutate(sentiment = fct_reorder(sentiment, n))

ggplot(data = ipcc_nrc_n) +
  geom_col(aes(x = sentiment, y = n))+
  coord_flip()
```

For each sentiment bin, what are the top 5, most frequent words associated with that bin?

```{r}
ipcc_nrc_n5 <- ipcc_nrc %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

ggplot(data = ipcc_nrc_n5, 
       aes(x = reorder(word, n), 
           y = n),
       fill = sentiment)+
  geom_col(show.legend = FALSE,
           aes(fill = sentiment))+
  facet_wrap(~sentiment, ncol = 2, scales = "free")
```






